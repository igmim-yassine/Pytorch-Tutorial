{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03E_LanguageModelling_ROCDataset_NLP_class_Emines_Jan22.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igmim-yassine/Pytorch-Tutorial/blob/master/03E_LanguageModelling_ROCDataset_NLP_class_Emines_Jan22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook Summary\n",
        "* Learn a Language Model on the ROC Story Dataset: https://cs.rochester.edu/nlp/rocstories/\n",
        "> Available here: https://drive.google.com/file/d/1eJINcSbC3JLl0hTNbhh5G94zTuXinpC-/view?usp=sharing\n",
        "\n",
        "* Generate Text with this Language Model using several decoding techniques\n",
        "* Evaluate the Language Model using the perplexity and the BLEU score. "
      ],
      "metadata": {
        "id": "E35Va__CQ04h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import re\n",
        "import json"
      ],
      "metadata": {
        "id": "6v5u6xAJRm40"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Load and Preprocess the Dataset"
      ],
      "metadata": {
        "id": "92Qmjnv8RRlt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTiJALLECm0u"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/MyDrive/12_Teaching/UM6P-NLP-Jan2022/notebooks/ROCStories_winter2017.csv\"\n",
        "df = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentences(df, max_samples=None):\n",
        "    df[\"sentence_1_2\"] = df.sentence1 + \" \" + df.sentence2\n",
        "    sentences = df.sentence_1_2\n",
        "    sentences_1, sentences_2 = df[\"sentence1\"], df[\"sentence2\"]\n",
        "    if max_samples is not None:\n",
        "        sentences = sentences[:max_samples]\n",
        "        sentences_1 = sentences_1[:max_samples]\n",
        "        sentences_2 = sentences_2[:max_samples]\n",
        "    return sentences, sentences_1, sentences_2"
      ],
      "metadata": {
        "id": "N0BluoGfDv-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, sentences_1, sentences_2 = get_sentences(df)"
      ],
      "metadata": {
        "id": "yWal4b8hRW-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print sentences example:\n"
      ],
      "metadata": {
        "id": "pqzrPZ_9SNLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 1**: Create a function `clean_text` that clean sentences\n",
        "* split words with \"-\"\n",
        "* split number and text using a regular expressions and the function `re.split`\n",
        "* Replace the token \"&\" by the token \"and\". \n",
        "* Tips: create lambda functions and apply it to the dataframe using `.apply` method. "
      ],
      "metadata": {
        "id": "36FAXVWNR37P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(sentences):\n",
        "    clean_func1 = lambda t:\n",
        "    clean_func2 = lambda t:\n",
        "    clean_func3 = lambda t:\n",
        "    clean_func4 = lambda t:\n",
        "    sentences = sentences.apply(clean_func1)\n",
        "    # ... #\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "G-UbiKEbD_iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = clean_text(sentences)\n",
        "sentences_1 = clean_text(sentences_1)\n",
        "sentences_2 = clean_text(sentences_2)"
      ],
      "metadata": {
        "id": "wdaxDDRyT3Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 2**: Build the vocab by removing some punctuation and adding the special tokens. "
      ],
      "metadata": {
        "id": "4GsDk0SkS-wG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXPa_EsRULDU",
        "outputId": "d6ecdfa7-4fa8-44eb-fc03-ca58f9a1406a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(sentences, tokens_to_remove=[\"$\", \"%\", \"'\", \"''\"], special_tokens=[\"<PAD>\", \"<SOS>\", \"<EOS>\"]):\n",
        "    print(\"Building vocab....\")\n",
        "    # tokenize sentences\n",
        "\n",
        "\n",
        "    # build vocab\n",
        "\n",
        "\n",
        "    print(\"vocab length:\", len(vocab))\n",
        "    print(\"saving vocab...\")\n",
        "    with open(\"vocab.json\", \"w\") as f:\n",
        "        json.dump(vocab, f)\n",
        "    return tokens, vocab"
      ],
      "metadata": {
        "id": "5AWJMCv2EBiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens, vocab = get_vocab(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYiFrF5DUTXM",
        "outputId": "f1377c06-a92b-4444-cc33-3a91b88fdde8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building vocab....\n",
            "vocab length: 20111\n",
            "saving vocab...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentences, vocab):\n",
        "\n",
        "    return df, len_sentences"
      ],
      "metadata": {
        "id": "pDesaF9CEYZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, len_sentences = tokenize(sentences, vocab)"
      ],
      "metadata": {
        "id": "kzMP2JlgUysi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_test(sentences, vocab):\n",
        "    tokenize_func = lambda t: word_tokenize(t)\n",
        "    tok_to_id_func = lambda t: [vocab[\"<SOS>\"]]+[vocab[w] for w in t if w in vocab.keys()]+[vocab[\"<EOS>\"]]\n",
        "    tokenized_sentences = sentences.apply(tokenize_func)\n",
        "    tokens_id = tokenized_sentences.apply(tok_to_id_func)\n",
        "    len_sentences = tokens_id.apply(len)\n",
        "    return tokens_id, len_sentences"
      ],
      "metadata": {
        "id": "LE1cfJmcEc3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_train_test(sentences, sentences_1_and_2, val_size=5000, test_size=3000):\n",
        "\n",
        "    return train_sentences, val_sentences, test_sentences"
      ],
      "metadata": {
        "id": "Xg1VZ2ZdFFsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data_path):\n",
        "    df = load_data(data_path)\n",
        "    sentences, sentences_1, sentences_2 = get_sentences(df)\n",
        "    sentences, sentences_1, sentences_2 = clean_text(sentences), clean_text(sentences_1), clean_text(sentences_2)\n",
        "    tokens, vocab = get_vocab(sentences)\n",
        "    padded_sentences, len_sentences = tokenize(sentences, vocab)\n",
        "    print(\"dataset set length:\", len(padded_sentences))\n",
        "    sentences_1, len_sentences_1 = tokenize_test(sentences_1, vocab)\n",
        "    sentences_2, len_sentences_2 = tokenize_test(sentences_2, vocab)\n",
        "    sentences_1_and_2 = pd.concat([sentences_1, sentences_2], axis=1)\n",
        "    train_sentences, val_sentences, test_sentences = split_train_test(padded_sentences, sentences_1_and_2)\n",
        "    print(\"train dataset size\", len(train_sentences))\n",
        "    print(\"val dataset size\", len(val_sentences))\n",
        "    print(\"test dataset size\", len(test_sentences))\n",
        "    return train_sentences, val_sentences, test_sentences"
      ],
      "metadata": {
        "id": "6PatJzbzFQcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(dataset, max_samples, batch_size):\n",
        "    input_sentence = np.array([seq for seq in dataset.input_sentence.values])\n",
        "    target_sentence = np.array([seq for seq in dataset.target_sentence.values])\n",
        "    if max_samples is not None:\n",
        "      input_sentence = input_sentence[:max_samples]\n",
        "      target_sentence = target_sentence[:max_samples]\n",
        "    tfdataset = tf.data.Dataset.from_tensor_slices(\n",
        "            (input_sentence, target_sentence))\n",
        "    dataloader = tfdataset.batch(batch_size, drop_remainder=True)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "XLhNTRFUF2xI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_dataloader(data):\n",
        "    inputs, targets = data\n",
        "    inputs = inputs.to_list()\n",
        "    targets = targets.to_list()\n",
        "    inputs = [tf.constant(inp, dtype=tf.int32) for inp in inputs]\n",
        "    targets = [tf.constant(tar, dtype=tf.int32) for tar in targets]\n",
        "    return (inputs, targets)"
      ],
      "metadata": {
        "id": "U4nIvqkUGwCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Exercise 4***: \n",
        "Create a `decode` function that decode a list of tokens id into text using the vocab. "
      ],
      "metadata": {
        "id": "PR8eRggwWjEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(self, seq_idx, vocab, delim=' ', ignored=[\"<SOS>\", \"<PAD>\"]):\n"
      ],
      "metadata": {
        "id": "PjbSjR7FOgU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 5**: Build a LSTM network with: \n",
        "* An embedding layer\n",
        "* A LSTM layer: there is a subtility -> you need to ouput the whole sequence of hidden states using the return_sequences argument: https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
        "* A dropout layer after the LSTM Layer\n",
        "* A dense layer that project the hidden state over the vocabulary. \n",
        "> What is the size of the NN output ? "
      ],
      "metadata": {
        "id": "rd_it0rCW9Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "def build_LSTM(seq_len, vocab_size, emb_size, output_size, rnn_units, dropout_rate, rnn_drop_rate=0.0):\n",
        "\n",
        "  return lstm_model"
      ],
      "metadata": {
        "id": "qW00GI4fL2XA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 6:**  \n",
        "Create a function that train LSTM (similarly of notebook of day 2)\n",
        "Compute the perplexity over the train and validation set: Note that the perplexity is the exponantial of the cross-entropy ! "
      ],
      "metadata": {
        "id": "L2hnzIxqXryK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_LSTM(model, optimizer, EPOCHS, train_dataset, val_dataset, output_path, checkpoint_path):\n",
        "    LSTM_ckpt_path = checkpoint_path + '/' + 'LSTM-{epoch}'\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=LSTM_ckpt_path,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            save_weights_only=True,\n",
        "            verbose=1)\n",
        "    ]\n",
        "\n",
        "    return train_history"
      ],
      "metadata": {
        "id": "BLcZGZufMK3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 7**:\n",
        "Create a function that generate text at inference over the trained lstm. \n",
        "This function either use: \n",
        "* greedy decoding using `tf.math.argmax`\n",
        "* sampling with temperature decoding `tf.random.categorical`"
      ],
      "metadata": {
        "id": "1r91Tz69YDcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(lstm, inputs, seq_len=10,\n",
        "                            decoding=\"sampling\", temp=1):\n",
        "  # Loop over number of decoding timesteps: (equal to seq_len)\n",
        "\n",
        "      # pass forward on the lstm \n",
        "\n",
        "      # get the last prediction (logits)\n",
        "\n",
        "      # if decoding = sampling \n",
        "        # divide logits by temperature \n",
        "        # sample a word\n",
        "\n",
        "        # if decoding == \"greedy\"\n",
        "        # find the greedy word (argmax)\n",
        "\n",
        "      # compute the inputs of the next timestep by concatenating inputs and the predicted token using tf.concat\n",
        "\n",
        "  # return the final inputs (complete sequence of word ids)\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "4ER0_eQqMhog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 8**:\n",
        "* Take an `inputs` of the test dataset, generate text on this inputs, and decode it with the `decode` function"
      ],
      "metadata": {
        "id": "G_sG6xjCZWFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Measuring the BLEU score \n",
        "(between true sentence and generated sentence on the test dataset)\n",
        "use sentence_bleu of nltk: https://www.nltk.org/_modules/nltk/translate/bleu_score.html"
      ],
      "metadata": {
        "id": "TEADJ9QWZpvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
      ],
      "metadata": {
        "id": "K8yT-n0oQl5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BLEU_score(true_sentence, generated_sentence, split_str=False):\n",
        "    if split_str:\n",
        "        true_sentence = true_sentence.split(sep=' ')\n",
        "        generated_sentence = [generated_sentence.split(sep=' ')]\n",
        "    score = sentence_bleu(references=generated_sentence, hypothesis=true_sentence, smoothing_function=SmoothingFunction().method2)\n",
        "    return score"
      ],
      "metadata": {
        "id": "xJA3l8IIQkS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise 9**: Create a function that: \n",
        "* Loop over the test set \n",
        "* generate text on each inputs of the test set\n",
        "* decode it using the decode function \n",
        "* Evaluate the BLEU score between the true decoded sentence (from the test set) and the decoded generate sentence \n",
        "* Compute the average BLEU score on the test set. "
      ],
      "metadata": {
        "id": "kkJkI4-_Z0gN"
      }
    }
  ]
}