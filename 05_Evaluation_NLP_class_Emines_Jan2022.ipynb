{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05_Evaluation_NLP-class-Emines-Jan2022.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/igmim-yassine/Pytorch-learning/blob/master/05_Evaluation_NLP_class_Emines_Jan2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation for the NLP class @ Emines (January 2022)\n",
        "\n",
        "### Exercise 1: Improving the performances of a sentiment analysis classifier\n",
        "**From the [notebook of day 2](https://colab.research.google.com/drive/1m3DSxqi3tnBBe6niKpggKw0i2uo1rgs7?usp=sharing), try to improve the performances of the sentiment analysis classifier by trying the following changes in the model/algorithm/preprocessing procedure**.  \n",
        "To evaluate the performances, look at the validation accuracy. \n",
        "\n",
        "1. Preprocessing: in the original dataset, remove stop_words and do some stemming (see https://www.nltk.org/howto/stem.html documentation)\n",
        "> Does it improve performance ? \n",
        "\n",
        "2. Model architecture changes: \n",
        "    * Replace the trainable embedding layer by an embedding layer which loads GloVe embeddings (see TP of day #1). \n",
        "    * Use several layers of LSTM\n",
        "    * Add a Dense Layer after the LSTM and before the final Dense Projection Layer \n",
        "    * Use a bi-directionnal LSTM using https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional\n",
        "    * Take the average of the sequence of hidden states as the sentence encoding (instead of the final hidden state)\n",
        "> Do you find an architecture that improves the performance of the classifier ? \n",
        "\n",
        "3. Training algorithm / hyper-parameters search\n",
        "*  add gradient clipping to limit exploding gradients: https://www.tensorflow.org/api_docs/python/tf/clip_by_norm\n",
        "* do hyperparameter-search on model dimensions, dropout rate, training hyper-parameters (learning rate, batch size) to reach a better val accuracy.\n",
        "> Which set of hyper-parameters work the best ? \n"
      ],
      "metadata": {
        "id": "mzydIbRJVUIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2: Learning a Language Model from scratch \n",
        "Learn a Language Model from the text [\"metamorphosis.txt\"](https://drive.google.com/file/d/1ip_8SZt0eYqkQMivtbTfnNiEqImtJS2t/view?usp=sharing) (dataset of notebook of day 1) and generate text with it. \n",
        "1. Preprocess your dataset for your needs (split into sentences, tokenize, clean text, build vocabulary, encode sentences)\n",
        "2. Create tensorflow datasets and dataloader from the processed datasets \n",
        "3. Build a RNN Language Model (see [notebook of day #3](https://colab.research.google.com/drive/1M_KpWjtcOrFZ_Ng8qUPHB0XTAZVWGCwE?usp=sharing)) and train it \n",
        "4. Compute its perplexity over the test set: can we obtain a perplexity around or inferior to 3 in the validation set ?\n",
        "5. Generate text with it using nucleus sampling (or top-k nucleus sampling): See this chunk of code as an help to implement the method: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "and print the generated text.\n",
        "6. Bonus Question: Take a random sentence of the test dataset, generate text with the trained language model, and compare it with text generated with GPT-2. \n",
        "See hugging face tutorial: https://huggingface.co/blog/how-to-generate\n"
      ],
      "metadata": {
        "id": "Qj3ibEQCYr8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3: Train a Sequence to Sequence Model with Attention on the ROC Story Dataset\n",
        "**Using the ROC Story Dataset of day #3 (lien [ici](https://drive.google.com/file/d/1eJINcSbC3JLl0hTNbhh5G94zTuXinpC-/view?usp=sharing)), build a sequence to sequence model with attention that takes as input sentence the sentence #1 (input of the encoder), and takes as target sentence (input of the decoder), the sentence #2**.   \n",
        "This creates an encoder-decoder model for story continuation.\n",
        "For the model, you can use either: \n",
        "1. The Seq2Seq RNN Model with attention of [notebook of day #4](https://colab.research.google.com/drive/1GPnhw6iQzVSMPvr8-dU8n3gJD67b9Upr?usp=sharing)\n",
        "> You can tweak the model, for example using a Multiplicative Attention instead of an Additive Attention\n",
        "2. A Transformer model. There is a great tutorial and transformer implementation in tensorflow here: https://www.tensorflow.org/text/tutorials/transformer"
      ],
      "metadata": {
        "id": "0_RdiCwzcwAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CPTKzueHWn_8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}